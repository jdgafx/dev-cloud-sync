name: Comprehensive Testing Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday 2 AM

env:
  PYTHON_VERSION: "3.10"
  NODE_VERSION: "18"

jobs:
  # Security and License Checks
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run security scan
        run: |
          pip install bandit safety
          bandit -r src/ -f json -o bandit-report.json
          safety check --json --output safety-report.json

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: "*-report.json"

  # Code Quality Checks
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run code formatting checks
        run: |
          black --check src/ tests/
          isort --check-only src/ tests/

      - name: Run linting
        run: |
          flake8 src/ tests/
          pylint src/ --output-format=json > pylint-report.json || true

      - name: Run type checking
        run: |
          mypy src/ --json-report mypy-report || true

      - name: Upload code quality reports
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: "*-report.json"

  # Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run unit tests with coverage
        run: |
          pytest tests/unit/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-fail-under=85 \
            --junitxml=unit-test-results-${{ matrix.python-version }}.xml \
            -v

      - name: Upload coverage reports
        uses: actions/upload-artifact@v3
        with:
          name: coverage-reports-python-${{ matrix.python-version }}
          path: |
            coverage.xml
            htmlcov/

      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results-python-${{ matrix.python-version }}
          path: unit-test-results-${{ matrix.python-version }}.xml

  # Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, code-quality]

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      minio:
        image: minio/minio:latest
        env:
          MINIO_ROOT_USER: minioadmin
          MINIO_ROOT_PASSWORD: minioadmin123
        command: server /data --console-address ":9001"
        options: >-
          --health-cmd "curl -f http://localhost:9000/minio/health/live"
          --health-interval 30s
          --health-timeout 20s
          --health-retries 3
        ports:
          - 9000:9000
          - 9001:9001

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Wait for services
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:5432; do sleep 2; done'
          timeout 60 bash -c 'until redis-cli ping; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:9000/minio/health/live; do sleep 2; done'

      - name: Run integration tests
        run: |
          pytest tests/integration/ \
            --junitxml=integration-test-results.xml \
            -v \
            --timeout=300
        env:
          DATABASE_URL: postgresql://postgres:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379/0
          S3_ENDPOINT: http://localhost:9000
          S3_ACCESS_KEY: minioadmin
          S3_SECRET_KEY: minioadmin123
          S3_BUCKET: test-bucket

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results
          path: integration-test-results.xml

  # Cross-Platform Deployment Tests
  deployment-tests:
    runs-on: ubuntu-latest
    needs: [integration-tests]

    strategy:
      matrix:
        distro: [
          "ubuntu:20.04",
          "ubuntu:22.04",
          "ubuntu:24.04",
          "debian:11",
          "debian:12",
          "centos:8",
          "fedora:38"
        ]

    steps:
      - uses: actions/checkout@v4

      - name: Run deployment test for ${{ matrix.distro }}
        run: |
          chmod +x scripts/deployment-test.sh
          ./scripts/deployment-test.sh --distro ${{ matrix.distro }}

      - name: Upload deployment test results
        uses: actions/upload-artifact@v3
        with:
          name: deployment-test-results-${{ matrix.distro }}
          path: test-results/

  # End-to-End and UX Tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: [deployment-tests]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Playwright
        run: |
          npm install -g @playwright/test
          npx playwright install chromium

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Start application for testing
        run: |
          python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 30  # Wait for application to start

      - name: Run UX tests
        run: |
          python tests/ux_test_framework.py
        env:
          BASE_URL: http://localhost:8000

      - name: Run E2E tests
        run: |
          npx playwright test tests/e2e/ --reporter=html

      - name: Upload UX test reports
        uses: actions/upload-artifact@v3
        with:
          name: ux-test-reports
          path: test-results/ux-reports/

      - name: Upload E2E test reports
        uses: actions/upload-artifact@v3
        with:
          name: e2e-test-reports
          path: playwright-report/

  # Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: [e2e-tests]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Start application
        run: |
          python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 30

      - name: Run performance tests
        run: |
          pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json

      - name: Run load tests
        run: |
          locust -f tests/load/locustfile.py \
            --headless \
            --users 10 \
            --spawn-rate 2 \
            --run-time 60s \
            --host http://localhost:8000 \
            --html load-test-report.html

      - name: Upload performance reports
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-reports
          path: |
            benchmark-results.json
            load-test-report.html

  # OpenSpec Validation
  openspec-validation:
    runs-on: ubuntu-latest
    needs: [code-quality]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run OpenSpec validation
        run: |
          python tests/openspec_validator.py .

      - name: Upload OpenSpec reports
        uses: actions/upload-artifact@v3
        with:
          name: openspec-validation-reports
          path: test-results/openspec-reports/

  # Accessibility Tests
  accessibility-tests:
    runs-on: ubuntu-latest
    needs: [e2e-tests]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install accessibility tools
        run: |
          npm install -g axe-cli
          npm install -g pa11y-ci

      - name: Start application
        run: |
          python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 30

      - name: Run accessibility tests
        run: |
          axe http://localhost:8000 --tags wcag2a,wcag2aa
          pa11y-ci --sitemap http://localhost:8000/sitemap.xml

      - name: Upload accessibility reports
        uses: actions/upload-artifact@v3
        with:
          name: accessibility-test-reports
          path: |
            axe-report.json
            pa11y-ci-report.json

  # Generate Comprehensive Report
  generate-reports:
    runs-on: ubuntu-latest
    needs: [
      security-scan,
      code-quality,
      unit-tests,
      integration-tests,
      deployment-tests,
      e2e-tests,
      performance-tests,
      openspec-validation,
      accessibility-tests
    ]
    if: always()

    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v3

      - name: Generate comprehensive test report
        run: |
          python scripts/generate_comprehensive_report.py

      - name: Upload final report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: comprehensive-test-report.html

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const reportContent = fs.readFileSync('test-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ§ª Test Results\n\n${reportContent}`
            });